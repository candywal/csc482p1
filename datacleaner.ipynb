{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa66e1a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bs4\n",
      "  Downloading bs4-0.0.1.tar.gz (1.1 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: beautifulsoup4 in /Users/anshul/Library/Python/3.10/lib/python/site-packages (from bs4) (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/anshul/Library/Python/3.10/lib/python/site-packages (from beautifulsoup4->bs4) (2.3.2.post1)\n",
      "Building wheels for collected packages: bs4\n",
      "  Building wheel for bs4 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for bs4: filename=bs4-0.0.1-py3-none-any.whl size=1257 sha256=71b028799fdf27fe901191016ccb6b35fa3aa87c98ecac7cd5748606a251a1ff\n",
      "  Stored in directory: /Users/anshul/Library/Caches/pip/wheels/25/42/45/b773edc52acb16cd2db4cf1a0b47117e2f69bb4eb300ed0e70\n",
      "Successfully built bs4\n",
      "Installing collected packages: bs4\n",
      "Successfully installed bs4-0.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96d3c49d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /opt/homebrew/lib/python3.10/site-packages (2.28.1)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/homebrew/lib/python3.10/site-packages (from requests) (1.26.13)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/anshul/Library/Python/3.10/lib/python/site-packages (from requests) (3.4)\r\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/homebrew/lib/python3.10/site-packages (from requests) (2.1.1)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.10/site-packages (from requests) (2022.9.24)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6ecad59",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = \"\"\"\n",
    "proj1F21_1412_A.html\n",
    "proj1F21_1412_B.html\n",
    "proj1F21_1422_A.html\n",
    "proj1F21_1422_B.html\n",
    "proj1F21_1483_A.html\n",
    "proj1F21_1483_B.html\n",
    "proj1F21_1794_A.html\n",
    "proj1F21_1794_B.html\n",
    "proj1F21_1901_A.html\n",
    "proj1F21_1901_B.html\n",
    "proj1F21_2404_A.html\n",
    "proj1F21_2404_B.html\n",
    "proj1F21_2422_A.html\n",
    "proj1F21_2422_B.html\n",
    "proj1F21_2509_A.html\n",
    "proj1F21_2509_B.html\n",
    "proj1F21_2592_A.html\n",
    "proj1F21_2592_B.html\n",
    "proj1F21_2605_A.html\n",
    "proj1F21_2605_B.html\n",
    "proj1F21_2925_A.html\n",
    "proj1F21_2925_B.html\n",
    "proj1F21_3061_A.html\n",
    "proj1F21_3061_B.html\n",
    "proj1F21_3363_A.html\n",
    "proj1F21_3363_B.html\n",
    "proj1F21_3453_A.html\n",
    "proj1F21_3453_B.html\n",
    "proj1F21_3490_A.html\n",
    "proj1F21_3490_B.html\n",
    "proj1F21_3509_A.html\n",
    "proj1F21_3509_B.html\n",
    "proj1F21_3550_A.html\n",
    "proj1F21_3550_B.html\n",
    "proj1F21_3618_A.html\n",
    "proj1F21_3618_B.html\n",
    "proj1F21_3647_A.html\n",
    "proj1F21_3647_B.html\n",
    "proj1F21_3928_A.html\n",
    "proj1F21_3928_B.html\n",
    "proj1F21_3942_A.html\n",
    "proj1F21_3942_B.html\n",
    "proj1F21_4001_A.html\n",
    "proj1F21_4001_B.html\n",
    "proj1F21_4079_A.html\n",
    "proj1F21_4079_B.html\n",
    "proj1F21_6218_A.html\n",
    "proj1F21_6218_B.html\n",
    "proj1F21_6388_A.html\n",
    "proj1F21_6388_B.html\n",
    "proj1F21_6533_A.html\n",
    "proj1F21_6533_B.html\n",
    "proj1F21_6585_A.html\n",
    "proj1F21_6585_B.html\n",
    "proj1F21_6728_A.html\n",
    "proj1F21_6728_B.html\n",
    "proj1F21_6737_A.html\n",
    "proj1F21_6737_B.html\n",
    "proj1F21_8630_A.html\n",
    "proj1F21_8630_B.html\n",
    "proj1F21_8842_A.html\n",
    "proj1F21_8842_B.html\n",
    "proj1F21_8980_A.html\n",
    "proj1F21_8980_B.html\n",
    "proj1F21_9348_A.html\n",
    "proj1F21_9348_B.html\n",
    "proj1F21_9578_A.html\n",
    "proj1F21_9578_B.html\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f67e6a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'proj1F21_1412_A.html', 'proj1F21_1412_B.html', 'proj1F21_1422_A.html', 'proj1F21_1422_B.html', 'proj1F21_1483_A.html', 'proj1F21_1483_B.html', 'proj1F21_1794_A.html', 'proj1F21_1794_B.html', 'proj1F21_1901_A.html', 'proj1F21_1901_B.html', 'proj1F21_2404_A.html', 'proj1F21_2404_B.html', 'proj1F21_2422_A.html', 'proj1F21_2422_B.html', 'proj1F21_2509_A.html', 'proj1F21_2509_B.html', 'proj1F21_2592_A.html', 'proj1F21_2592_B.html', 'proj1F21_2605_A.html', 'proj1F21_2605_B.html', 'proj1F21_2925_A.html', 'proj1F21_2925_B.html', 'proj1F21_3061_A.html', 'proj1F21_3061_B.html', 'proj1F21_3363_A.html', 'proj1F21_3363_B.html', 'proj1F21_3453_A.html', 'proj1F21_3453_B.html', 'proj1F21_3490_A.html', 'proj1F21_3490_B.html', 'proj1F21_3509_A.html', 'proj1F21_3509_B.html', 'proj1F21_3550_A.html', 'proj1F21_3550_B.html', 'proj1F21_3618_A.html', 'proj1F21_3618_B.html', 'proj1F21_3647_A.html', 'proj1F21_3647_B.html', 'proj1F21_3928_A.html', 'proj1F21_3928_B.html', 'proj1F21_3942_A.html', 'proj1F21_3942_B.html', 'proj1F21_4001_A.html', 'proj1F21_4001_B.html', 'proj1F21_4079_A.html', 'proj1F21_4079_B.html', 'proj1F21_6218_A.html', 'proj1F21_6218_B.html', 'proj1F21_6388_A.html', 'proj1F21_6388_B.html', 'proj1F21_6533_A.html', 'proj1F21_6533_B.html', 'proj1F21_6585_A.html', 'proj1F21_6585_B.html', 'proj1F21_6728_A.html', 'proj1F21_6728_B.html', 'proj1F21_6737_A.html', 'proj1F21_6737_B.html', 'proj1F21_8630_A.html', 'proj1F21_8630_B.html', 'proj1F21_8842_A.html', 'proj1F21_8842_B.html', 'proj1F21_8980_A.html', 'proj1F21_8980_B.html', 'proj1F21_9348_A.html', 'proj1F21_9348_B.html', 'proj1F21_9578_A.html', 'proj1F21_9578_B.html']\n"
     ]
    }
   ],
   "source": [
    "print(names.splitlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abd73500",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "[Errno 17] File exists: './txtfiles/proj1F21_1412_A.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(soup\u001b[38;5;241m.\u001b[39mstripped_strings)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m names\u001b[38;5;241m.\u001b[39msplitlines()[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[0;32m---> 19\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./txtfiles/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtxt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     20\u001b[0m         f\u001b[38;5;241m.\u001b[39mwrite(remove_tags(name))\n\u001b[1;32m     21\u001b[0m         f\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/homebrew/opt/ipython/libexec/lib/python3.10/site-packages/IPython/core/interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    277\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m     )\n\u001b[0;32m--> 282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: './txtfiles/proj1F21_1412_A.txt'"
     ]
    }
   ],
   "source": [
    "# Import Module\n",
    "from bs4 import BeautifulSoup\n",
    " \n",
    "# Function to remove tags\n",
    "def remove_tags(html):\n",
    " \n",
    "    # parse html content\n",
    "    with open(\"./proj1F21_files/\" + html) as fp:\n",
    "        soup = BeautifulSoup(fp)\n",
    " \n",
    "    for data in soup(['style', 'script']):\n",
    "        # Remove tags\n",
    "        data.decompose()\n",
    " \n",
    "    # return data by retrieving the tag content\n",
    "    return ' '.join(soup.stripped_strings)\n",
    " \n",
    "for name in names.splitlines()[1:]:\n",
    "    with open(\"./txtfiles/\"+name[:-4] + \"txt\", \"x\") as f:\n",
    "        f.write(remove_tags(name))\n",
    "        f.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ed4b65c",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './txtfiles/proj1F21_*.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m textfiles \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./txtfiles/proj1F21_*.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m documents \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m textiles:\n",
      "File \u001b[0;32m/opt/homebrew/opt/ipython/libexec/lib/python3.10/site-packages/IPython/core/interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    277\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m     )\n\u001b[0;32m--> 282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './txtfiles/proj1F21_*.txt'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "textfiles = names.splitlines()[1:]:\n",
    "documents = []\n",
    "\n",
    "for file in textiles:\n",
    "    with open(\"./txtfiles/\"+name[:-4] + \"txt\", \"x\") as f\n",
    "        content = file.read()\n",
    "    documents.append(content)\n",
    "\n",
    "labels = []\n",
    "\n",
    "for file in textiles:\n",
    "    if \"A\" in file:\n",
    "        labels.append(\"person\")\n",
    "    else:\n",
    "        labels.append(\"vehicle\")\n",
    "\n",
    "documents_shuffled, labels_shuffled = shuffle(documents, labels)\n",
    "documents_shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190cf452",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the documents and labels into training and testing sets\n",
    "train_documents, test_documents = documents_shuffled[:int(len(documents_shuffled)*0.8)], documents_shuffled[int(len(documents_shuffled)*0.8):]\n",
    "train_labels, test_labels = labels_shuffled[:int(len(labels_shuffled)*0.8)], labels_shuffled[int(len(labels_shuffled)*0.8):]\n",
    "\n",
    "#Create a list of words\n",
    "words = [word.lower() for sentence in train_documents for word in nltk.word_tokenize(sentence)]\n",
    "\n",
    "#Create a list of unique words\n",
    "unique_words = list(set(words))\n",
    "\n",
    "#Create a dictionary mapping unique words to a unique integer\n",
    "word2int = {word: i for i, word in enumerate(unique_words)}\n",
    "\n",
    "#Create a list of documents represented as vectors\n",
    "train_data = []\n",
    "\n",
    "#Loop through each document\n",
    "for document in train_documents:\n",
    "    #Create a vector with all 0s\n",
    "    vector = np.zeros(len(unique_words))\n",
    "    #Loop through each word in the document\n",
    "    for word in nltk.word_tokenize(document):\n",
    "        #Update the vector with the index of the word\n",
    "        vector[word2int[word.lower()]] += 1\n",
    "    #Add the vector to the list of documents\n",
    "    train_data.append(vector)\n",
    "\n",
    "#Create a list of labels for the training data\n",
    "train_labels_data = np.array([1 if label == \"person\" else 0 for label in train_labels])\n",
    "\n",
    "#Create a list of documents represented as vectors\n",
    "test_data = []\n",
    "\n",
    "#Loop through each document\n",
    "for document in test_documents:\n",
    "    #Create a vector with all 0s\n",
    "    vector = np.zeros(len(unique_words))\n",
    "    #Loop through each word in the document\n",
    "    for word in nltk.word_tokenize(document):\n",
    "        #Update the vector with the index of the word\n",
    "        vector[word2int[word.lower()]] += 1\n",
    "    #Add the vector to the list of documents\n",
    "    test_data.append(vector)\n",
    "\n",
    "#Create a list of labels for the testing data\n",
    "test_labels_data = np.array([1 if label == \"person\" else 0 for label in test_labels])\n",
    "\n",
    "#Train a Naive Bayes classifier\n",
    "classifier = nltk.NaiveBayesClassifier.train(zip(train_data, train_labels_data))\n",
    "\n",
    "#Test the accuracy of the classifier\n",
    "accuracy = nltk.classify.accuracy(classifier, zip(test_data, test_labels_data))\n",
    "\n",
    "print(\"The accuracy of the Naive Bayes classifier is:\", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
